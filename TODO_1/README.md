# Hadoop (Apache)
link: https://hadoop.apache.org/

The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.

# Hadoop HDFS

link: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#Introduction

The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is now an Apache Hadoop subproject. The project URL is https://hadoop.apache.org/hdfs/.

# Ansible and chef

link: https://www.upguard.com/blog/ansible-vs-chef#:~:text=Ansible%20and%20chef%20are%20configuration,of%20new%20servers%20from%20scratch.

Ansible and chef are configuration management (CM) tools that help sysadmins and DevOps professionals manage a large number of servers. They excel at repetitive task automation, simultaneous deployment of apps and packages to a group of servers or configuration and provisioning of new servers from scratch

# Scala

link: https://www.scala-lang.org/

Scala combines object-oriented and functional programming in one concise, high-level language. Scala's static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries.

# Apache spark

link: https://spark.apache.org/

pache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.

# Dask

link: https://www.dask.org/

Dask is a flexible library for parallel computing in Python.

Dask is composed of two parts:

Dynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.

“Big Data” collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.
